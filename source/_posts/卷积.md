---
title: 卷积
date: 2019-03-19 16:07:30
categories: 
- 春招准备
tags:
- 卷积
- 深度学习
- 基础知识 
---
### 卷积 
- [可参考](https://flat2010.github.io/2018/06/15/%E6%89%8B%E7%AE%97CNN%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0/)

> 卷积核的参数是共享可以使得图像上的内容不受位置的影响，即图片中的猫不管是在图片的左上角或者右下角都应该被分类为猫，同时共享卷积核的参数可以大幅减少网络模型中的参数，降低运算的复杂度。
<!--more-->
作者：损失函数
链接：https://www.imooc.com/article/23802
来源：慕课作网
本文原创发布于慕课网 ，转载请注明出处，谢谢合

### 卷积层参数计算
- 单层卷积层总参数＝卷积核个数*(卷积核尺寸+偏置项)
- 比如64个3*3的卷积核，输入通道为3：64(3*3*3+1)=1792
- 一个卷积核的参数=卷积核大小*通道数

### feature map 大小计算
- 卷积：o =（i+2p-k）/s +1
- 反卷积：o′=s(i−1)+k−2p

### CNN
- NLP是对一维信号、词序列做处理；计算机视觉是对二维或者三维（比如视频）做处理。
- 特点：
  - 区域不变性：滤波器的滑动，检测的是局部特征；pooling是综合局特征的；过滤了位置信息，图像中猫的位置在哪里我不关心这种就很适用，但是比如NLP中词的顺序重要的话，可能就稍微不太适用，要做特殊的处理？
  - 局部组合性：低层的局部特征组合成高层的全局化的特征
  - CNN的一大特性是用**局部连接**代替了**全连接**，适用于图像是因为**图像的特征是局部强相关**的，而**文本也有这样的特性**。
  - CNN假设是数据在二维空间有局部相关性，RNN假设数据在时序空间存在相关性；文本在时序空间有相关性，1维空间是2维空间的特例；文本和语音常用RNN，手写体也有时序相关，所以也可以是RNN。

### 好处
- 统计角度：抓住了问题的局部相关性和空间不变性（像素点与周围像素点有关联，不能随意打乱的数据，具有局部相关性；低层次的特征到高层次的特征，有空间不变性）
- 正则化角度：局部链接、权值共享、降低了模型参数数量、控制了复杂度（一个卷积核提取一种特征）
- 神经科学角度：受生物视觉系统启发

### 什麽样的资料集不适合用深度学习?
- 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。
- 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

作者：许韩
链接：https://www.zhihu.com/question/41233373/answer/145404190
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

