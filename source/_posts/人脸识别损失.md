---
title: 人脸识别损失
date: 2019-06-13 14:55:54
tags:
---


##### 1 人脸识别一般步骤：
- 找人脸
- 对齐人脸
- 识别（损失函数用于这一步骤）

<!--more-->

##### 2 CosFace Sphere Face ArcFace
- 都是训练时对卷积神经网络提出了更高的目标。

###### 2.1 SoftMax
>1 Softmax函数
在数学，尤其是概率论和相关领域中，Softmax函数，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量$\mathbf{z}$   “压缩”到另一个K维实向量$\sigma(\mathbf{z})$中，使得每一个元素的范围都在$(0,1)$之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出(数据类别j的概率)：
$$ \sigma(\mathbf{z})_{j} =\frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}  for j = 1, …, K.$$
softmax 将一组向量进行压缩，使得到的向量各元素之和为 1，而压缩后的值便可以作为置信率，所以常用于分类问题。另外，在实际运算的时候，为了避免上溢和下溢，在将向量丢进softmax之前往往先对每个元素减去其中最大值。
为什么要减去最大值：https://zhuanlan.zhihu.com/p/29376573
$$ \sigma(\mathbf{z})_{j} =\frac{e^{z_j-z_{max}}}{\sum_{k=1}^{K}e^{z_k-z_{max}}}  for j = 1, …, K.$$

>2 Softmax Loss
上面丢入$K$维的$\mathbf{z}$向量，得到了$\sigma(\mathbf{z})$，softmax Loss就是：
$$SL=\sum_{k=1}^{K}-y_klog(\sigma_k)$$
$y_k$是个one hot的向量，只有1个1，为groundtruth,所以上面的式子可以变成：
$$SL=-y_{gt}log(\sigma_{gt})=-log(\sigma_{gt})$$


>3 交叉熵
交叉熵的公式是：
$$CE=\sum_{k=1}^{K}-P_klog(p_k)$$
其中P为真实分布，p为预测分布，对应到上面的例子得到：
$$CE=\sum_{k=1}^{K}-P_klog(p_k)=\sum_{k=1}^{K}-y_klog(\sigma_k)=-log(\sigma_{gt})$$
所以其实Softmax Loss等价于Softmax层＋交叉熵损失，即：
$$Softmax Loss=Cross Entropy(Softmax)$$


###### 2.2 Sphere Face

1 基础知识：向量点乘$a.b=|a||b|cos(\theta)$
2 要想增强softmax的分类能力,分布上做到两点：（1）同类距离更近（2）不同类距离更远
3 Softmax Loss：
$$L=-log(\sigma_{gt})=-log(\frac{e^{z_{gt}}}{\sum_{k=1}^{K}e^{z_{k}}})=-log(\frac{e^{ W^T_{gt}x+b_{gt}   }}{\sum_{k=1}^{K}e^{ W^T_{k}x+b_{k}  }}) =-log(\frac{e^{ ||W_{gt}|| ||x|| cos(\theta_{W_{gt}, x})+b_{gt}   }}{\sum_{k=1}^{K}e^{||W_{k}|| ||x|| cos(\theta_{W_{k}, x})+b_{k}  }})  $$

将$W_k$归一化，并将偏置$b$置为0，得到$L_m$,modified：
$$L_m=-log(\frac{e^{  ||x|| cos(\theta_{W_{gt}, x})   }}{\sum_{k=1}^{K}e^{ ||x|| cos(\theta_{W_{k}, x})  }})  $$

4 用二分类来讨论这个问题，这里softmax loss 的决策边界是（$W1$ $W2$代表类别1、类别2）：
$$(W_1-W_2)x+b_1-b_2=0$$

然后我们上面令W=1,b=0,上式就变成：
$$||x||(cos(\theta_1-\theta_2))=0$$

为了让类1的特征$x$被正确分类，修改后的softmax 损失函数要求$cos(\theta_1)>cos(\theta_2)$,即$x$属于类1，$\theta1<\theta2$。

sphere face函数加了一个参数m(m>=2),这个时候还要正确分类的话，就是：
$$cos(m\theta_1)>cos(\theta_2)$$
也就是$\theta_1<\theta_2/m$

m是>=1的整数
$$L_{ang}=-log(\frac{e^{  ||x|| cos(m\theta_{W_{gt}, x})   }}{\sum_{k=1}^{K}e^{ ||x|| cos(m\theta_{W_{k}, x})  }})  m=1,2,3,4,.... $$ 


参考：
https://zhuanlan.zhihu.com/p/60747096

https://www.twblogs.net/a/5b80e7722b71772165aa0712/zh-cn


#### ArcFace

下面两张图主要是ArcFace和TripletLoss 和 Softmax Loss的对比。简单来讲Arcface是：
- Softmax Loss 加上了Margin
- Triplet Loss 从Image和Image的对比，换成了Image和Class的对比。
 <div style="width: 400px; margin: auto">![avater](1.jpg)</div>
 <div style="width: 400px; margin: auto">![avater](2.jpg)</div>




























