---
title: 人脸识别损失
date: 2019-06-13 14:55:54
tags:
---


##### 1 人脸识别一般步骤：
- 找人脸
- 对齐人脸
- 识别（损失函数用于这一步骤）

<!--more-->

##### 2 CosFace Sphere Face ArcFace
- 都是训练时对卷积神经网络提出了更高的目标。

###### 2.1SoftMax
>1 Softmax函数
在数学，尤其是概率论和相关领域中，Softmax函数，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量$\mathbf{z}$   “压缩”到另一个K维实向量$\sigma(\mathbf{z})$中，使得每一个元素的范围都在$(0,1)$之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出：
$$ \sigma(\mathbf{z})_{j} =\frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}  for j = 1, …, K.$$
softmax 将一组向量进行压缩，使得到的向量各元素之和为 1，而压缩后的值便可以作为置信率，所以常用于分类问题。另外，在实际运算的时候，为了避免上溢和下溢，在将向量丢进softmax之前往往先对每个元素减去其中最大值。
为什么要减去最大值：https://zhuanlan.zhihu.com/p/29376573
$$ \sigma(\mathbf{z})_{j} =\frac{e^{z_j-z_{max}}}{\sum_{k=1}^{K}e^{z_k-z_{max}}}  for j = 1, …, K.$$

>2 Softmax Loss
上面丢入$K$维的$\mathbf{z}$向量，得到了$\sigma(\mathbf{z})$，softmax Loss就是：
$$SL=\sum_{k=1}^{K}-y_klog(\sigma_k)$$
$y_k$是个one hot的向量，只有1个1，为groundtruth,所以上面的式子可以变成：
$$SL=-y_{gt}log(\sigma_{gt})=-log(\sigma_{gt})$$


>3 交叉熵
交叉熵的公式是：
$$CE=\sum_{k=1}^{K}-P_klog(p_k)$$
其中P为真实分布，p为预测分布，对应到上面的例子得到：
$$CE=\sum_{k=1}^{K}-P_klog(p_k)=\sum_{k=1}^{K}-y_klog(\sigma_k)=-log(\sigma_{gt})$$
所以其实Softmax Loss等价于Softmax层＋交叉熵损失，即：
$$Softmax Loss=Cross Entropy(Softmax)$$



参考：
https://zhuanlan.zhihu.com/p/60747096
