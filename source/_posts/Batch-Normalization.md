---
title: Batch Normalization
date: 2019-03-19 15:13:14
categories: 
- 春招准备
tags:
- BN
- 深度学习
- 基础知识 
---
### 参考来源
- [github](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese)
- [知乎](https://www.zhihu.com/question/275788133/answer/386749776)

### 背景
- Batch Normalization, 这里的batch指的是批数据，**批标准化**, 和普通的数据标准化类似,
- 是将**分散的数据统一**的一种做法, 也是优化神经网络的一种方法.**加快收敛** 提升网络稳定性
- 神经网络在初始阶段已经不对那些比较大的 x 特征范围 **敏感**了. 这样很糟糕
- 想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别, 这就证明我的感官系统失效了
- 就是想到了隐藏层，能不能对隐藏层的输入做向之前那样的normalization
- 训练的本质是**学习数据分布**。如果训练数据与测试数据的分布不同会降低模型的泛化能力。因此，应该在开始训练前对所有输入数据做归一化处理。
- 而在神经网络中，因为每个隐层的参数不同，会使下一层的输入发生变化，从而导致每一批数据的**分布也发生改变**；致使网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与过拟合的风险。
- 方差是标准差的算术平方根

### 作用
- 加速网络的训练（缓解梯度消失，支持更大的学习率）
- 防止过拟合
- 降低了参数初始化的要求。
> - BN的**核心思想不是为了防止梯度消失或者防止过拟合**，其核心是通过对**系统参数搜索空间进行约束**来增加系统鲁棒性，这种约束压缩了搜索空间，约束也改善了系统的结构合理性，这会带来一系列的性能改善，比如加速收敛，保证梯度，缓解过拟合等。
 - 在深度网络拟合一个（未知）函数的时候，比如图像识别，实际上满足任务要求的**拟合曲线**（如果将深度网络的映射过程看作一个时序的不同数据空间之间的映射）**有无穷多**，大家的判别性能都基本相当，但是这些不同的拟合（曲线）其鲁棒性是不同的，或者说网络对输入数据点映射得到的曲线在数据空间的分布模式是不同的。我们希望的映射，或者说直觉上，好的映射曲线应该是让数据曲线的**分布尽量均匀尽量平滑**，之所以出现梯度消失就是对不同数据点的**映射曲线在某些位置出现了过于集中**（不同输入点映射到几乎相同的点），过拟合也是曲线出现了异常的部分集中。
 - 使用BN，就是要保证曲线分布尽量均匀平滑均匀（数据的各个维度上尺度一致从而**避免出现某些维度数据过于集中**），这可以带来缓解梯度消失和过拟合
 - 作者：信息门下走狗  链接：https://www.zhihu.com/question/275788133/answer/386749776  来源：知乎

### 原理
- 针对每一批数据，在网络的每一层输入之前增加归一化处理，使输入的均值为 0，标准差为 1，**约束数据分布**
- 但同时 BN 也降低了模型的拟合能力，破坏了之前学到的特征分布；所以有个反向的操作，将normalize后的数据扩展与平移，让网络自己学着去改gamma和平移beta，判断normalize有没有用，自己决定normalize的尺度，尝试还原最优的原来的数据分布。
### 训练
![avatar](TIM截图20190319151834.jpg)

### 测试
![avatar](TIM截图20190319153814.jpg)

### 总算法
![avatar](TIM截图20190319151846.jpg)


