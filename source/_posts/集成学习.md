---
title: 集成学习
date: 2019-04-04 15:10:10
tags:
- 知识整理
---

# 1 集成学习概述
- 描述：多个学习器组合成性能更好的学习器。
- 原因：不同模型在测试集有不同误差，平均来讲，集成模型能至少与任一成员表现一致；若**成员误差独立**，集成模型结果要显著好。

# 2 集成学习分类
### 1 Boosting
- **串行**策略，学习器之间有**依赖**关系，新的学习器需要依赖旧的学习器生成。
- 举例：AdaBoost、GBDT
- **需要解决问题**：
  - 每一轮如何改变数据的权值或概率分布？？？
  - 如何将弱分类器组合成强分类器？
<!--more-->

### 2 Bagging
- **并行**策略，不存在依赖，基学习器可以**同时生成**。
- 举例：随机森林、Dropout


### 3 Stacking
- 作者说介绍不多，有时间再整理，这个是啥？？

# 3 AdaBoost 算法

### 他对两个基本问题的解决
- 每一轮如何改变数据的权值或者概率分布呢？
  - **数据样本权值的处理**：开始时，样本的权值是一样的；后来，提高上一轮弱分类器错误分类样本的权值，降低被正确分类样本的权值。
- 如何将弱分类器组合成强分类器呢？
  - **加权表决**：加大分类误差率小的基学习器的权值；减小分类错误率大的基学习器的权值。

### AdaBoost 算法描述

- 输入：训练集 `T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ {-1,+1}`，基学习器 `G1(x)`
- 输出：最终学习器 `G(x)`

1. 初始化训练数据的权值分布,这里的w1i中的1，表示第一次w的取值。
    $$D_1=(w_{1,1},\cdots,w_{1,i},\cdots,w_{1,N}),w_{1,i}=\frac{1}{N},i=1,2,\cdots,N)$$
2. 对 `m=1,2,..,M`  （m）应该表示第几个基学习器吧。
    - 使用这一轮的训练集，得到这一轮的基分类器。
    - 计算这个分类器的分类误差率，实际上分类错误率等于**所有分类错误的数据的权值之和**
    - 得到这个分类器的系数（ln(1-em)/em）,也就是错误越大，系数越小，也就是他的重要性。
    - 更新训练集的权值分布，使得他变成一个类似于softmax的概率分布。α相当于这一轮的一个系数。**目的就是被分错的样本的权重大，分对的权重小**。
<div style="width: 800px; margin: auto">![avater](1.png)</div>
<div style="width: 800px; margin: auto">![avater](1.5.png)</div>


> 博主给的解释：
- 开始时，训练集中所有数据具有均匀的权值分布
- 计算分类误差率，实际上就是计算所有分类错误的数据的权值之和
- `G_m(x)` 的系数 `α_m` 表示该学习器在最终学习器中的重要性；公式 $\alpha_m=\frac{1}{2}\ln\frac{1-e_m}{e_m}$ 表明当分类错误率 `e_m <= 1/2` 时，`α_m >= 0`，并且 `α_m` 随 `e_m` 的减小而增大
- 被基分类器分类错误的样本权值会扩大，而分类正确的权值会缩小——**不改变训练数据，而不断改变训练数据权值的分布，使训练数据在基学习器的学习中起到不同的作用**，这是 AdaBoost 的一个特点。

# 4 前向分步算法
- **AdaBoost 算法是前向分步算法的特例**。
- 此时，基函数为基分类器，损失函数为指数函数`L(y,f(x)) = exp(-y*f(x))`
<div style="width: 900px; margin: auto">![avater](2.png)</div>
<div style="width: 900px; margin: auto">![avater](3.png)</div>

# 5 GBDT
- GBDT的基分类器是决策树
- 与提升树的区别：提升树使用的是真正的**残差**，梯度提升树使用当前模型的**负梯度**来拟合残差。

### 提升树，Boosting Tree
- 基分类器：决策树，二叉分类树或者二叉回归树。
- 回归问题时，不断拟合残差得到新的树。
- 提升树可以表示为**决策树的加法模型**：$f_M(x)=\sum_{m=1}^MT(x;\Theta_m)$
- 首先初始化提升树 `f_0(x)=0`，则第 m 步的模型为 $f_m(x)=f_{m-1}(x)&plus;T(x;\Theta_m)$
- 然后通过最小化损失函数决定下一个决策树的参数**注意学习的是残差**
$$\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^NL(y_i,{f_{m-1}(x_i)+T(x_i;\Theta_m)})$$
- 对于二分类问题，提升树算法只需要将[AdaBoost 算法](#adaboost-算法描述)中的基学习器限制为二叉分类树即可

### 提升树算法描述
在回归问题中，新的树是通过不断拟合**残差**（residual）得到的。
- 输入：训练集 `T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ R`
- 输出：回归提升树 `f_M(x)`

1. 初始化 `f_0(x)=0`  
1. 对 `m=1,2,..,M`
   i. 计算**残差**：${r_{m,i}}=y_i-f_{m-1}(x_i),i=1,2,..,N $
   ii. **拟合残差**学习下一个回归树的参数 :$\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^N L({\color{Red} r_{m,i}},{T(x_i;\Theta_m)})"$
   iii. 更新 `f_m(x)`   $f_m(x)=f_{m-1}(x)&plus;T(x;\Theta_m)$
1. 得到回归提升树 $f_M(x)=\sum_{m=1}^MT(x;\Theta_m)$


### 梯度提升(GB)算法
- 当损失函数为平方损失或指数损失时，每一步的优化是很直观的；但对于一般的损失函数而言，不太容易——梯度提升正是针对这一问题提出的算法；
- 梯度提升是梯度下降的近似方法，其关键是利用损失函数的**负梯度作为残差的近似值**，来拟合下一个决策树。

### GBDT 算法描述
- 输入：训练集 `T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ R`；损失函数 `L(y,f(x))`；
- 输出：回归树 `f_M(x)`
1  初始化回归树
$$f_0(x)={\color{Red}\arg\underset{c}{\min}}\sum_{i=1}^NL(y_i,c)$$
2 对 `m=1,2,..,M`
    i. 对 `i=1,2,..,N`，计算残差/负梯度: 
    $$r_{m,i}=-\frac{\partial L(y_i,{\color{Red} f_{m-1}(x_i)}))}{\partial {\color{Red} f_{m-1}(x_i)}}$$  
    ii. 对 `r_mi` 拟合一个回归树，得到第 `m` 棵树的叶节点区域:
    $$R_{m,j},\quad j=1,2,..,J$$
    iii. 对 `j=1,2,..,J`，计算:
    $$c_{m,j}={\color{Red} \arg\underset{c}{\min}}\sum_{x_i\in R_{m,j}}L(y_i,{\color{Blue} f_{m-1}(x_i)&plus;c})$$
    iiii. 更新回归树:
    $$f_m(x)=f_{m-1}&plus;\sum_{j=1}^J c_{m,j}{\color{Blue} I(x\in R_{m,j})}$$

3  得到回归树:
$$f_M(x)=\sum_{i=1}^M\sum_{j=1}^Jc_{m,j}{\color{Blue} I(x\in R_{m,j})}$$

- 说明：
  - 算法第 1 步初始化，估计使损失函数最小的常数值，得到一棵只有一个根节点的树
  - 第 2(i) 步计算损失函数的负梯度，将其作为残差的估计
    - 对平方损失而言，负梯度就是残差；对于一般的损失函数，它是残差的近似
  - 第 2(ii) 步估计回归树的节点区域，以拟合残差的近似值
  - 第 2(iii) 步利用线性搜索估计叶节点区域的值，使损失函数最小化

# XGBoost 算法(再看看)
> [一步一步理解GB、GBDT、xgboost](https://www.cnblogs.com/wxquare/p/5541414.html) - wxquare - 博客园 
- XGBoost 是改进的[梯度提升(GB)算法](#梯度提升GB算法)；
- [XGBoost 库](https://github.com/dmlc/xgboost)是 XGBoost 算法的高效实现


# Refernce
- [imhuhuay的Github](https://github.com/ava-YangL/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/C-%E4%B8%93%E9%A2%98-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.md)，https://github.com/imhuay/Algorithm_Interview_Notes-Chinese
