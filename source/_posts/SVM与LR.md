---
title: SVM与LR
date: 2019-04-08 23:32:50
tags:
---
## 1 相同
- 都是分类算法，除了SVR。。。
- 如果不考虑核函数，LR和SVM都是线性分类算法，分类决策面都是线性的。（LR和SVM其实都可以用核函数），决策树可是非线性模型。
- 都是监督学习算法。
- 都是辨别模型。（辨别模型生成一个P(Y|X)的预测模型，生成模型计算联合概率分布P(Y,X)利用**贝叶斯公式**转化为条件概率），也就是辨别模型不会计算联合概率分布，生成模型更厉害一些，直接去找这个数据是怎么生成的。
<!--more-->
>生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别。
https://www.cnblogs.com/zhizhan/p/5038747.html

https://www.cnblogs.com/peizhe123/p/5674730.html


## 2 不同
1 损失函数不同：
- LR基于概率理论，假设样本为1的概率可用sigmoid函数表示，用极大似然方法估计参数值。
- LR 交叉熵损失。
<div style="width: 400px; margin: auto">![avater](1.png)</div>
- SVM基于几何间隔最大化，最大几何间隔的分类面是最优分类面。
- SVM hinge Loss
<div style="width: 400px; margin: auto">![avater](2.png)</div>
2 SVM只考虑局部的边界附近的点(支持向量)，LR考虑全局，会受到每个样本点的影响。**数据敏感程度**
>线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。​（引自http://www.zhihu.com/question/26768865/answer/34078149）
3 非线性问题的解决：SVM采用核函数，LR通常不采用核函数。
- 因为SVM计算分类面的时候用了少数代表支持向量的样本，也就是说只有他们参与了核计算。LR每个样本都参与决策面的计算，如果都用核函数，复杂度很高。所以在具体应用的时候，LR很少用到核函数机制。
4 线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响（SVM基于距离分类，LR基于概率分类）
5 SVM的损失函数 1/2||w||^2，自带正则，也就是他结构风险最小化的原因；LR必须在损失函数上添加正则项。
- 结构风险最小化：训练误差和模型复杂度之间的平衡，防止过拟合。
6 在小规模数据集上，Linear SVM要略好于LR，但差别也不是特别大，而且Linear SVM的计算复杂度受数据量限制，对海量数据LR使用更加广泛。

#### Hinge Loss

1  实现了软间隔分类（这个Loss函数都可以做到）
2  保持了支持向量机解的稀疏性换用其他的Loss函数的话，SVM就不再是SVM了。
  正是因为Hinge Loss的零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这才是支持向量机最大的优势所在，对训练样本数目的依赖大大减少，而且提高了训练效率。

作者：檀画
链接：https://www.zhihu.com/question/47746939/answer/154058298
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

<div style="width: 600px; margin: auto">![avater](3.png)</div>
<div style="width: 600px; margin: auto">![avater](4.png)</div>

#### LR与SVM的区别与联系 
现在回过头来理解“LR和SVM本质不同在于loss function的不同“这句话，也就不难了，LR的损失函数是Cross entropy loss，svm的损失函数是Hinge loss，两个模型都属于线性分类器，而且性能相当。区别在于
- LR的解是受数据本身分布影响的，而SVM的解不受数据分布的影响；
- LR的输出具有自然的概率意义，而SVM的输出不具有概率意义；
- SVM依赖数据表达的距离测度，所以在建模前需要对数据标准化，LR则不需要；
- SVM受惩罚系数C的影响较大，实验中需要做Validation，LR则不需要；
- LR适合于大样本学习，SVM适合于小样本学习。
换用其他的Loss函数的话，SVM就不再是SVM了。正是因为Hinge Loss的**零区域对应的正是非支持向量的普通样本**，从而所有的普通样本都不参与最终超平面的决定，这才是支持向量机最大的优势所在，对训练样本数目的依赖大大减少，而且提高了训练效率。

作者：徐小贱民
链接：https://zhuanlan.zhihu.com/p/31403164
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。




## 3 逻辑斯蒂回归

### 定义及推导

- 可参考https://blog.csdn.net/ligang_csdn/article/details/53838743
- 我写的真的像狗屎一样乱.....
<div style="width: 600px; margin: auto">![avater](24.jpg)</div>

### 多分类的逻辑斯蒂回归（TODO）

## 4 支持向量机

### 支持向量机简述
- 支持向量机（Support Vector Machines, SVM）是一种二分类模型。它的**基本模型**是定义在特征空间上的**间隔最大**的线性分类器，间隔最大使它有别于感知机；支持向量机还包括**核技巧**，这使其成为实质上的非线性分类器。
- **SVM 的学习策略就是间隔最大化**，可形式化为一个求解**凸二次规划**的问题，也等价于正则化的**合页损失函数**的最小化问题。
- SVM 的最优化算法是求解凸二次规划的最优化算法。

#### 什么是支持向量
- 训练数据集中与分离超平面距离最近的样本点的实例称为支持向量
- 更通俗的解释：
  - 数据集种的某些点，位置比较特殊。比如 `x+y-2=0` 这条直线，假设出现在直线上方的样本记为 A 类，下方的记为 B 类。
  - 在寻找找这条直线的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。
  - 这些点就是所谓的“支持点”，在数学中，这些点称为**向量**，所以更正式的名称为“**支持向量**”。
  > [SVM中支持向量的通俗解释](https://blog.csdn.net/AerisIceBear/article/details/79588583) - CSDN博客 

#### 支持向量机的分类
- 线性可分支持向量机
  - 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个线性分类器，即线性可分支持向量机，又称**硬间隔支持向量机**。
- 线性支持向量机
  - 当训练数据**接近线性可分**时，通过**软间隔最大化**，学习一个线性分类器，即线性支持向量机，又称**软间隔支持向量机**。
- 非线性支持向量机
  - 当训练数据**线性不可分**时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机。

#### 核函数与核技巧
- **核函数**表示将输入从输入空间映射到特征空间后得到的特征向量之间的内积

#### 最大间隔超平面背后的原理
> 机器学习技法 (1-5) - 林轩田
- 相当于在**最小化权重**时对训练误差进行了约束——对比 L2 范数正则化，则是在最小化训练误差时，对权重进行约束

<div style="width: 300px; margin: auto">![avater](25.png)</div>
  > 与 L2 正则化的区别
- 相当于**限制了模型复杂度**——在一定程度上防止过拟合，具有更强的泛化能力

## 支持向量机推导
- SVM 由简至繁包括：**线性可分支持向量机**、**线性支持向量机**以及**非线性支持向量机**
  
### 线性可分支持向量机推导
> 《统计学习方法》 & [支持向量机SVM推导及求解过程](https://blog.csdn.net/american199062/article/details/51322852#commentBox) - CSDN博客
- 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个线性分类器，即线性可分支持向量机，又称**硬间隔支持向量机**。
- 线性 SVM 的推导分为两部分
  1. 如何根据**间隔最大化**的目标导出 SVM 的**标准问题**；
  1. 拉格朗日乘子法对偶问题的求解过程.

### 思路
使用SVM算法的思路：（1）简单情况，线性可分情况，把问题转化为一个凸优化问题，可以用拉格朗日乘子法简化，然后用既有的算法解决；（2）复杂情况，线性不可分，用核函数将样本投射到高维空间，使其变成线性可分的情形，利用核函数来减少高纬度计算量。 

### 推导（再看看）

