---
title: BP算法
date: 2019-03-19 14:20:24
categories: 
- 春招准备
tags:
- BP
- 深度学习
- 基础知识 
---

### 概述啊
- **梯度下降法**：利用**损失函数对所有参数的梯度**寻找局部最小值点。
- **BP**：用于计算该梯度的具体方法，其本质是利用**链式法则**对每个参数求**偏导**。
- **实际意义**：但其在实际运算中的意义比链式法则要大的多。
<!--more-->

### 怎么去理解BP
- 人们用多层神经网络来表示（X,Y）之间的复杂关系，多层神经网络本质是一个多层复合的函数。BP是求解这个多层复合函数的所有变量的偏导数的集合。

![avatar](TIM截图20190319140337.jpg)

- 梯度下降法：给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到cost收敛。
### 举例

- e=(a+b)*(b+1) ,链式法则，路径上偏导值的乘积，如下图所示：

![avatar](TIM截图20190319145745.jpg)

- 这样做很冗余，因为a-c-e，b-c-e都走了c-e路径。
- 同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。（**反向寻找路径**）
> - 从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放"些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以"层"为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。
 - 以上图为例，节点c接受e发送的1*2并堆放起来，节点d接受e发送的1*3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5.
 - 作者：Anonymous
 链接：https://www.zhihu.com/question/27239198/answer/89853077
 来源：知乎
 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



## 参考来源：
- [github](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese)
- [知乎](https://www.zhihu.com/question/27239198?rf=24827633)



