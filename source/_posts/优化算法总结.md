---
title: 优化算法总结
date: 2019-03-26 15:23:11
categories: 
- 机器学习
tags:
- 优化算法
- 面试准备
- 春招准备 
---
参考链接：https://zhuanlan.zhihu.com/p/32626442
梯度下降是目前神经网络中使用最为广泛的优化算法之一。

为什么梯度反方向是函数值局部下降最快的方向？ https://zhuanlan.zhihu.com/p/24913912
1 导数：可以当成曲线上的切线斜率，或者函数在该点的变化率。
直白的来说，导数代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量变化的比值代表了导数，几何意义有该点的切线。物理意义有该时刻的（瞬时）变化率...

2 偏导数：原来我们学到的偏导数指的是多元函数沿坐标轴的变化率，但是我们往往很多时候要考虑多元函数沿任意方向的变化率，那么就引出了方向导数.

3 方向导数

4 梯度方向是函数变化率最大的方向，梯度是方向导数取得最大值的方向，有一种通俗的的理解，梯度是导数的高维形式，导数是增长的方向，所以梯度其实是增长的方向，那么，梯度的反方向就是增长的反方向，下降。