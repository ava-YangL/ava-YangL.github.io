---
title: 优化算法总结
date: 2019-03-26 15:23:11
categories: 
- 机器学习
tags:
- 优化算法 
---
参考链接：https://zhuanlan.zhihu.com/p/32626442
梯度下降是目前神经网络中使用最为广泛的优化算法之一。

为什么梯度反方向是函数值局部下降最快的方向？ https://zhuanlan.zhihu.com/p/24913912
1 导数：可以当成曲线上的切线斜率，或者函数在该点的变化率。
直白的来说，导数代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量变化的比值代表了导数，几何意义有该点的切线。物理意义有该时刻的（瞬时）变化率...

2 偏导数：原来我们学到的偏导数指的是多元函数沿坐标轴的变化率，但是我们往往很多时候要考虑多元函数沿任意方向的变化率，那么就引出了方向导数.

3 方向导数

4 梯度方向是函数变化率最大的方向，梯度是方向导数取得最大值的方向，有一种通俗的的理解，梯度是导数的高维形式，导数是增长的方向，所以梯度其实是增长的方向，那么，梯度的反方向就是增长的反方向，下降。


#### Adam算法
- 是随机梯度下降算法的扩展
- Adaptive moment estimation，适应性矩估计。
- 随机梯度下降保持**单一的学习率**更新所有的权重；Adam通过计算梯度的一阶矩估计和二阶矩估计为不同的参数设计独立的**自适应性学习率**。
- 是适应行梯度算法（Adagrad，为每个参数保留一个学习率以提升在稀疏梯度上的性能）和均方根传播（RMSProp，基于权重梯度最近量级的均值为每一个参数适应性地保留学习率，意味着算法在非稳态和在线问题上有很优秀的性能）的优点集合。
- 参数：alpha（学习率），beta1（一阶矩估计的指数衰减率，如0.9），beta2（二阶矩估计的指数衰减率，如0.999，该超参数在稀疏梯度（如cv，nlp）任务中应接近于1），epsilon（该参数非常小，如1E-8))。
- 计算了梯度的指数移动平均，beta1和beta2控制了移动均值的衰减率。

下图参考来自：https://segmentfault.com/a/1190000012668819
<div style="width: 600px; margin: auto">![avater](1.png)</div>

