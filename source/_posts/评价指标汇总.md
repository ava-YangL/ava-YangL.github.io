---
title: 评价指标汇总
date: 2019-08-06 11:34:23
tags:
---
# 1 分类

精确率/查准率 + 召回率/查全率 + 准确率 + F1 + ROC
<!--more-->
## 1 混淆矩阵
True Positive(真正, TP)：将正类预测为正类数.
True Negative(真负 , TN)：将负类预测为负类数.
False Positive(假正, FP)：将负类预测为正类数 → 误报 (Type I error).
False Negative(假负 , FN)：将正类预测为负类数 → 漏报 (Type II error).

## 2 精确率/查准率（precision）
精确率是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本 ： P=TP/(TP+FP)
我觉得这个是越大越好吧。

## 3 召回率/查全率 （recall）
召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了： R=TP/(TP+FN)
这个也是越大越好。

   > **在信息检索领域，精确率和召回率又被称为查准率和查全率**
   > 查准率＝检索出的相关信息量 / 检索出的信息总量 
   > 查全率＝检索出的相关信息量 / 系统中的相关信息总量
   > **总结一下就是：**
   > 准确率(accuracy) = 预测对的/所有 = (TP+TN)/(TP+FN+FP+TN) 
   > 精确率(precision) = TP/(TP+FP) 
   > 召回率(recall) = TP/(TP+FN) 

## 4 准确率（Accuracy）
ACC=(TP+TN)/(TP+TN+FP+FN)
> 在正负样本不平衡的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用acc，即使全部预测成负类（不点击）acc 也有 99% 以上，没有意义。

## 5 F1
是精确率和召回率的调和均值
$$F1=2(P×R)/(P+R)$$
$$\frac{2}{F1}=\frac{1}{P}+\frac{1}{R}$$
$$F1=\frac{2TP}{2TP+FP+FN}$$
F1分数认为召回率和精确率同等重要
> 我们使用调和平均而不是简单的算术平均的原因是：调和平均可以惩罚极端情况。一个具有 1.0 的精度，而召回率为 0 的分类器，这两个指标的算术平均是 0.5，但是 F1 score 会是 0。F1 score 给了精度和召回率相同的权重，它是通用 Fβ指标的一个特殊情况，在 Fβ中，β 可以用来给召回率和精度更多或者更少的权重。
$$Fβ=(1+β^2)\frac{P×R}{(β^2·P)+R}$$
> F1分数认为召回率和精确率同等重要，F2分数认为召回率的重要程度是精确率的2倍，而F0.5分数认为召回率的重要程度是精确率的一半。
> https://blog.csdn.net/oTengYue/article/details/89426004

## 6 ROC
比如在逻辑回归里面，我们会设一个阈值，大于这个值的为正类，小于这个值为负类。如果我们减小这个阀值，那么更多的样本会被识别为正类。这会提高正类的识别率，但同时也会使得更多的负类被错误识别为正类。为了形象化这一变化，在此引入 ROC ，ROC 曲线可以用于评价一个分类器好坏。
True positive rate:$ TPR=TP/(TP+FN) $ ÷所有的正例  **越大越好**
False positive rate:$ FPR=FP/(FP+TN) $ ÷所有的负例  **越小越好**
直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正率）和 FP（假正率）间的 trade-off2。

> 机器学习模型中，很多模型输出是预测概率。而使用精确率、召回率这类指标进行模型评估时，还需要对预测概率设分类阈值，比如预测概率大于阈值为正例，反之为负例。这使得模型多了一个超参数，并且这个超参数会影响模型的泛化能力。
> 当我们调整阈值时，就会造成不同的精准率和召回率，阈值越高(越少的被预测为正例)，精准率越高，召回率越低。阈值越低则相反。
> **接收者操作特征（Receiver Operating Characteristic，ROC）曲线不需要设定这样的阈值。ROC曲线纵坐标是真正率，横坐标是假正率**


(0,1) ：即 FPR=0, TPR=1，这意味着 FN（false negative）=0，并且FP（false positive）=0。这意味着分类器很完美，因为它将所有的样本都正确分类。 **所以左上角的点是最好的**
(1,0) ：即 FPR=1，TPR=0，这个**分类器是最糟糕**的，因为它成功避开了所有的正确答案。
(0,0) ：即 FPR=TPR=0，即 FP（false positive）=TP（true positive）=0，此时分类器将**所有的样本都预测为负样本（negative）**。
(1,1) ：**分类器将所有的样本都预测为正样本**。
对角线上的点（TPR=FRP） ：表示分类器将一半的样本猜测为正样本，另外一半的样本猜测为负样本。

**因此，ROC 曲线越接近左上角，分类器的性能越好。**


## 7 AUC
AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。
翻译过来就是，随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是 AUC 值。

> 简单说：AUC值越大的分类器，正确率越高3。
AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
AUC<0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC<0.5 的情况。

既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反）


> 是 ROC 曲线下的面积，它是一个数值，当仅仅看 ROC 曲线分辨不出哪个分类器的效果更好时，用这个数值来判断。
> 从上面定义可知，意思是随机挑选一个正样本和一个负样本，当前分类算法得到的 Score 将这个正样本排在负样本前面的概率就是 AUC 值。AUC 值是一个概率值，AUC 值越大，分类算法越好。


# 2 回归

平均绝对误差MAE（Mean Absolute Error）又被称为 l1 范数损失（l1-norm loss）： Sum|targ-pre| / num
平均平方误差 MSE（Mean Squared Error）又被称为 l2 范数损失（l2-norm loss）： Sum(targ-pre)^2 / num


参考：
https://www.zhihu.com/question/19645541
https://www.cnblogs.com/zhizhan/p/4870429.html
http://charleshm.github.io/2016/03/Model-Performance/
https://blog.csdn.net/oTengYue/article/details/89426004
https://www.jianshu.com/p/42bfe1a79d12