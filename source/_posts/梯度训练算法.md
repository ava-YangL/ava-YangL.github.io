---
title: 梯度训练算法
date: 2019-04-08 22:58:10
tags:
---


### Batch-GD
每次迭代的梯度方向由所有训练样本共同投票决定，计算的是损失函数在整个训练集上的梯度方向。

### Mini-batch-GD
每个mini batch有b个样本

### Stochastic- GD
每个mini batch有1个样本
<!--more-->
### OnLine-GD 
充分利用实时数据，所有训练数据只用一次，然后丢弃，随着数据的变化而迁移。

### 总结
    Batch-GD：所有样本跑后，更新权值，优点是精确，缺点是效率低。
    Stochastic-GD:一个样本，优点是快，缺点是容易找不到全局最优解。
    mini-batch GD： 比较中立的结果。
<div style="width: 600px; margin: auto">![avater](1.png)</div>